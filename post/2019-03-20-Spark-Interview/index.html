<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>Spark面试题搜集 | 闪光的沙粒</title>
<meta name="description" content="温故而知新">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://Bradley-dufe.github.io/favicon.ico?v=1561050285855">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://Bradley-dufe.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>

<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />



  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://Bradley-dufe.github.io">
        <img src="https://Bradley-dufe.github.io/images/avatar.png?v=1561050285855" class="site-logo">
        <h1 class="site-title">闪光的沙粒</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      温故而知新
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://Bradley-dufe.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">Spark面试题搜集</h2>
            <div class="post-date">2019-03-20</div>
            
            <div class="post-content">
              <h3 id="简答型题目">简答型题目</h3>
<h4 id="spark技术栈有哪些组件每个组件都有什么功能适合什么应用场景">Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h4>
<ul>
<li>
<p>Spark core：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。</p>
</li>
<li>
<p>SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。</p>
</li>
<li>
<p>Spark sql：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析</p>
</li>
<li>
<p>BlinkDB ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。</p>
</li>
<li>
<p>MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>
</li>
<li>
<p>GraphX是Spark中用于图和图并行计算</p>
</li>
</ul>
<h4 id="spark中driver的功能是什么">Spark中Driver的功能是什么</h4>
<ul>
<li>一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext的实例，是程序的人口点</li>
<li>功能：负责向集群申请资源，向master注册信息，负责了作业的调度，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGScheduler，TaskScheduler。</li>
</ul>
<h4 id="spark中worker的主要工作是什么">Spark中Worker的主要工作是什么</h4>
<ul>
<li>主要功能：管理当前节点内存，CPU的使用状况，接收master分配过来的资源指令，通过ExecutorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务。需要注意的是：
<ul>
<li>worker会不会汇报当前信息给master，worker心跳给master主要只有workid，它不会发送资源信息以心跳的方式给mater，master分配的时候就知道worker，只有出现故障的时候才会发送资源。</li>
<li>worker不会运行代码，具体运行的是Executor是可以运行具体appliaction写的业务逻辑代码，操作代码的节点，它不会运行程序的代码的。</li>
</ul>
</li>
</ul>
<h4 id="spark的有几种部署模式每种模式特点">Spark的有几种部署模式，每种模式特点</h4>
<ul>
<li>
<p>本地模式
Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类</p>
<ul>
<li>local[1]：只启动一个executor</li>
<li>local[k]:启动k个executor</li>
<li>local：启动跟cpu数目相同的 executor</li>
</ul>
</li>
<li>
<p>standalone模式
分布式部署集群， 自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础，</p>
</li>
<li>
<p>Spark on yarn模式
分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端</p>
</li>
<li>
<p>Spark On Mesos模式</p>
<p>官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序：</p>
</li>
</ul>
<h4 id="spark为什么比mapreduce快">Spark为什么比mapreduce快</h4>
<ul>
<li>基于内存计算，减少低效的磁盘交互；</li>
<li>高效的调度算法，基于DAG；</li>
<li>容错机制Linage，精华部分就是DAG和Lingae地方</li>
</ul>
<h4 id="hadoop和spark的shuffle相同和差异">hadoop和spark的shuffle相同和差异？</h4>
<ul>
<li>从 high-level 的角度来看，两者并没有大的差别。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</li>
<li>从 low-level 的角度来看，两者差别不小。 Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。</li>
<li>从实现角度来看，两者也有不少差别。 Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？ Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</li>
</ul>
<h4 id="rdd宽依赖和窄依赖">RDD宽依赖和窄依赖</h4>
<ul>
<li>窄依赖指的是每一个parent RDD的Partition最多被子RDD的一个Partition使用</li>
<li>宽依赖指的是多个子RDD的Partition会依赖同一个parent RDD的Partition</li>
</ul>
<h4 id="cache和pesist的区别">cache和pesist的区别</h4>
<ul>
<li>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间</li>
<li>cache只有一个默认的缓存级别MEMORY_ONLY ，cache调用了persist，而persist可以根据情况设置其它的缓存级别；</li>
<li>executor执行的时候，默认60%做cache，40%做task操作，persist最根本的函数，最底层的函数</li>
</ul>
<h4 id="常规的容错方式有哪几种类型rdd通过linage记录数据更新的方式为何很高效">常规的容错方式有哪几种类型？RDD通过Linage（记录数据更新）的方式为何很高效？</h4>
<ul>
<li>CheckPoint,会发生拷贝，浪费资源</li>
<li>记录数据的更新，每次更新都会记录下来，比较复杂且比较消耗性能
<ul>
<li>lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且rdd之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条将复杂计算链条存储下来，计算的时候从后往前回溯900步是上一个stage的结束，要么就checkpoint</li>
<li>记录原数据，是每次修改都记录，代价很大如果修改一个集合，代价就很小，官方说rdd是粗粒度的操作，是为了效率，为了简化，每次都是操作数据集合，写或者修改操作，都是基于集合的rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的也可以是细粒度，读可以读其中的一条条的记录。</li>
<li>简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景如网络爬虫，现实世界中，大多数写是粗粒度的场景</li>
</ul>
</li>
</ul>
<h4 id="rdd有哪些缺陷">RDD有哪些缺陷</h4>
<ul>
<li>不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是说可以一条条的读</li>
<li>不支持增量迭代计算，Flink支持</li>
</ul>
<h4 id="spark中数据的位置是被谁管理的">Spark中数据的位置是被谁管理的？</h4>
<p>每个数据分片都对应具体物理位置，数据的位置是被blockManager，无论数据是在磁盘，内存还是tacyan，都是由blockManager管理</p>
<h4 id="spark的数据本地性有哪几种">Spark的数据本地性有哪几种？</h4>
<ul>
<li>PROCESS_LOCAL是指读取缓存在本地节点的数据</li>
<li>NODE_LOCAL是指读取本地节点硬盘数据</li>
<li>ANY是指读取非本地节点数据通常读取数据</li>
<li>PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中</li>
</ul>
<h4 id="rdd有几种操作类型">RDD有几种操作类型</h4>
<ul>
<li>transformation：进行数据状态的转换，对已有的RDD创建新的RDD。</li>
<li>Action：触发具体的作业，对RDD最后取结果的一种操作</li>
<li>Controller：对性能效率和容错方面的支持。persist , cache, checkpoint</li>
</ul>
<h4 id="spark程序执行有时候默认为什么会产生很多task怎么修改默认task执行个数">Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？</h4>
<ul>
<li>因为输入数据有很多task，尤其是有很多小文件的时候，有多少个输入block就会有多少个task启动；</li>
<li>Spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率。不过task并不是越多越好，如果平时测试，或者数据量没有那么大，则没有必要task数量太多。</li>
<li>参数可以通过spark_home/conf/spark-default.conf配置文件设置:spark.sql.shuffle.partitions 50 spark.default.parallelism 10第一个是针对spark sql的task数量第二个是非spark sql程序设置生效</li>
</ul>
<h4 id="为什么spark-application在没有获得足够的资源job就开始执行了可能会导致什么什么问题发生">为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生?</h4>
<p>会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑否则很容易出现长时间分配不到资源，job一直不能运行的情况。</p>
<h4 id="join操作优化经验">join操作优化经验？</h4>
<p>join其实常见的就分为两类： map-side join 和  reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。可以将小表通过broadcast广播出去，转换为map-side-join操作。</p>
<h4 id="spark-如何防止内存溢出">spark 如何防止内存溢出</h4>
<ul>
<li>driver端的内存溢出
<ul>
<li>可以增大driver的内存参数：spark.driver.memory (default 1g)</li>
<li>这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。</li>
</ul>
</li>
<li>map过程产生大量对象导致内存溢出
<ul>
<li>这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。
面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区，不会有shuffle的过程。</li>
</ul>
</li>
<li>数据不平衡导致内存溢出
<ul>
<li>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。</li>
</ul>
</li>
<li>shuffle后内存溢出
<ul>
<li>shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。</li>
</ul>
</li>
<li>standalone模式下资源分配不均匀导致内存溢出
<ul>
<li>在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</li>
</ul>
</li>
<li>使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()
<ul>
<li>rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。</li>
</ul>
</li>
</ul>
<h4 id="spark中如何划分stage">Spark中如何划分Stage</h4>
<ul>
<li>
<p>Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是宽窄依赖，遇到宽依赖就划分stage，每个stage包含一个或多个task，然后将这些task以taskSet的形式提交给TaskScheduler运行，stage是由一组并行的task组成</p>
</li>
<li>
<p>Spark程序中可以因为不同的action触发众多的job，一个程序中可以有很多的job，每一个job是由一个或者多个stage构成的，后面的stage依赖于前面的stage，也就是说只有前面依赖的stage计算完毕后，后面的stage才会运行</p>
</li>
<li>
<p>stage 的划分标准就是宽依赖：何时产生宽依赖就会产生一个新的stage，例如reduceByKey,groupByKey，join的算子，会导致宽依赖的产生；</p>
</li>
<li>
<p>切割规则：从后往前，遇到宽依赖就切割stage</p>
</li>
</ul>
<h4 id="dagscheduler分析">DAGScheduler分析</h4>
<ul>
<li>
<p>是一个面向stage 的调度器；</p>
<ul>
<li>主要入参有：dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,resultHandler, localProperties.get)</li>
<li>rdd: final RDD</li>
<li>cleanedFunc: 计算每个分区的函数</li>
<li>resultHander:  结果侦听器</li>
</ul>
</li>
<li>
<p>主要功能：</p>
<ul>
<li>1.接受用户提交的job</li>
<li>2.将job根据类型划分为不同的stage，记录那些RDD，stage被物化，并在每一个stage内产生一系列的task，并封装成taskset；</li>
<li>3.决定每个task的最佳位置，任务在数据所在节点上运行，并结合当前的缓存情况，将taskSet提交给TaskScheduler；</li>
<li>4.重新提交shuffle输出丢失的stage给taskScheduler；</li>
</ul>
</li>
<li>
<p>注：一个stage内部的错误不是由shuffle输出丢失造成的，DAGScheduler是不管的，由TaskScheduler负责尝试重新提交task执行。</p>
</li>
</ul>
<h4 id="job的生成">Job的生成：</h4>
<p>​	一旦driver程序中出现action，就会生成一个job，比如count等，向DAGScheduler提交job，如果driver程序后面还有action，那么其他action也会对应生成相应的job，所以，driver端有多少action就会提交多少job，这可能就是为什么spark将driver程序称为application而不是job 的原因。每一个job可能会包含一个或者多个stage，最后一个stage生成result，在提交job 的过程中，DAGScheduler会首先从后往前划分stage，划分的标准就是宽依赖，一旦遇到宽依赖就划分，然后先提交没有父阶段的stage们，并在提交过程中，计算该stage的task数目以及类型，并提交具体的task，在这些无父阶段的stage提交完之后，依赖该stage 的stage才会提交。</p>
<h4 id="spark-master使用zookeeper进行ha的有哪些元数据保存在zookeeper">Spark master使用zookeeper进行HA的，有哪些元数据保存在Zookeeper？</h4>
<p>spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置，包括Worker，Driver和Application以及Executors。standby节点要从zk中，获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的。另外，Master切换需要注意2点</p>
<ul>
<li>在Master切换的过程中，所有的已经在运行的程序皆正常运行！因为Spark Application在运行前就已经通过Cluster Manager获得了计算资源，所以在运行时Job本身的调度和处理和Master是没有任何关系的！</li>
<li>在Master的切换过程中唯一的影响是不能提交新的Job：一方面不能够提交新的应用程序给集群，因为只有Active Master才能接受新的程序的提交请求；另一方面，已经运行的程序中也不能够因为action操作触发新的Job的提交请求</li>
</ul>
<h4 id="spark-master-ha-主从切换过程不会影响集群已有的作业运行为什么"><strong>Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？</strong></h4>
<p>答：因为程序在运行之前，已经申请过资源了，Driver和Executors通讯，不需要和Master进行通讯的。</p>
<h4 id="spark-on-mesos中什么是的粗粒度分配什么是细粒度分配各自的优点和缺点是什么">Spark on Mesos中，什么是的粗粒度分配，什么是细粒度分配，各自的优点和缺点是什么？</h4>
<ul>
<li>粗粒度：启动时就分配好资源， 程序启动，后续具体使用就使用分配好的资源，不需要再分配资源
<ul>
<li>好处：作业特别多时，资源复用率高，适合粗粒度；</li>
<li>不足：容易资源浪费，假如一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度，999个资源就会闲置在那里，资源浪费。</li>
</ul>
</li>
<li>细粒度分配：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。</li>
</ul>
<h4 id="sparksql解析过程">SparkSQL解析过程</h4>
<p>SQL语句首先通过Parser模块被解析为语法树，此棵树称为Unresolved Logical Plan；Unresolved Logical Plan通过Analyzer模块借助于Catalog中的表信息解析为Logical Plan；此时，Optimizer再通过各种基于规则的优化策略进行深入优化，得到Optimized Logical Plan；优化后的逻辑执行计划依然是逻辑的，并不能被Spark系统理解，此时需要将此逻辑执行计划转换为Physical Plan。Physical Plan再转换为Spark Plan和Prepared Spark Plan最后转换为RDD。</p>
<h4 id="cache后面能不能接其他算子它是不是action操作">cache后面能不能接其他算子,它是不是action操作</h4>
<p>cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。
cache不是action操作</p>
<h4 id="reducebykey是不是action"><strong>reduceByKey是不是action？</strong></h4>
<p>不是，很多人都会以为是action，reduce rdd是action</p>
<h4 id="rdd的弹性表现在哪几点">RDD的弹性表现在哪几点？</h4>
<ul>
<li>自动的进行内存和磁盘的存储切换；</li>
<li>基于Linage的高效容错；</li>
<li>task如果失败会自动进行特定次数的重试；</li>
<li>stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；</li>
<li>checkpoint和persist，数据计算之后持久化缓存</li>
<li>数据调度弹性，DAG TASK调度和资源无关</li>
<li>数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par</li>
</ul>

            </div>
            
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>



  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: 'e5c24a09874cf61b51ff',
        clientSecret: 'b277cd50d3a51aaa778d434f6243613b66d14020',
        repo: 'blog-comments',
        owner: 'Bradley-dufe',
        admin: ['Bradley-dufe'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
