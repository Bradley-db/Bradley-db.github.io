<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>闪光的沙粒</title>
<meta name="description" content="温故而知新" />
<link rel="shortcut icon" href="https://bradley-db.github.io/favicon.ico?v=1587572051124">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">

<link rel="stylesheet" href="https://bradley-db.github.io/styles/main.css">



  </head>
  <body>
    <div id="app" class="main px-4 flex flex-col lg:flex-row">
      <div id="sidebar" class="sidebar-wrapper lg:static lg:w-1/4">
  <div class="lg:sticky top-0">
    <div class="sidebar-content">
      <div class="flex lg:block p-4 lg:px-0 items-center fixed lg:static lg:block top-0 right-0 left-0 bg-white z-50">
        <i class="ri-menu-2-line lg:mt-4 text-2xl cursor-pointer animated fadeIn" onclick="openMenu()"></i>
        <a href="https://bradley-db.github.io">
          <img class="animated fadeInLeft avatar rounded-lg mx-4 lg:mt-32 lg:mx-0 mt-0 lg:w-24 lg:h-24 w-12 w-12" src="https://bradley-db.github.io/images/avatar.png?v=1587572051124" alt="">
        </a>
        <h1 class="animated fadeInLeft lg:text-4xl font-extrabold lg:mt-8 mt-0 text-xl" style="animation-delay: 0.2s">闪光的沙粒</h1>
      </div>
      
        <div class="animated fadeInLeft" style="animation-delay: 0.4s">
          <p class="my-4 text-gray-600 font-light hidden lg:block">
            文章目录
          </p>
          <div class="toc-container hidden lg:block">
            <ul class="markdownIt-TOC">
<li><a href="#mapreduce%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%E5%92%8Cspark%E5%AE%9E%E7%8E%B0">MapReduce经典问题和Spark实现</a></li>
</ul>

          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="menu-container">
  <i class="ri-arrow-left-line text-2xl cursor-pointer animated fadeIn close-menu-btn" onclick="closeMenu()"></i>
  <div>
    
      
        <a href="/" class="menu" style="animation-delay: 0s">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu" style="animation-delay: 0.2s">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu" style="animation-delay: 0.4s">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu" style="animation-delay: 0.6000000000000001s">
          关于
        </a>
      
    
  </div>
  <div class="site-footer">
    <div class="py-4 text-gray-700">Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a></div>
    <a class="rss" href="https://bradley-db.github.io/atom.xml" target="_blank">RSS</a>
  </div>
</div>
<div class="mask" onclick="closeMenu()">
</div>
      <div class="content-wrapper py-32 lg:p-8 lg:w-3/4 post-detail animated fadeIn">
        <h1 class="text-3xl font-bold lg:mt-16">MapReduce经典问题和Spark实现</h1>
        <div class="text-sm text-gray-700 lg:my-8">
          2019-04-21 / 4 min read
        </div>
        
          <img class="post-feature-image rounded-lg mx-auto my-4" src="https://bradley-db.github.io/post-images/2019-04-21-Classic-MapReduce-Problem-With-Spark.jpg" alt="">
        
        <div class="post-content yue">
          <h1 id="mapreduce经典问题和spark实现">MapReduce经典问题和Spark实现</h1>
<p>MapReduce计算范式是当今的分布式计算算法实现的基础，也是算法工程师需要掌握的知识，同时“面试造火箭“的常见问题。本文梳理了MapReduce常见的算法场景,并通过Spark进行实现</p>
<!-- more -->
<pre><code class="language-scala">package org.shiningsand

import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.ArrayBuffer
import scala.util.Random


/**
  * @Author: YangLei
  * @Description:
  * @Date: Create in 11:18 AM 2019/4/24
  * @update in 11:18 AM 2019/4/24
  */
object ClassicMRProblem extends Logging{
  val ss = SparkSession.builder()
    .appName(&quot;mr-implementation&quot;)
    .master(&quot;local[*]&quot;)
    .config(sparkConfig)
    .getOrCreate()
  ss.sparkContext.setLogLevel(&quot;error&quot;)
  val sc = ss.sparkContext

  def main(args: Array[String]): Unit = {
    topN()
  }
  def MaxMin: Unit = {


    //初始化测试数据(可以指定分区数）
    val data = sc.parallelize(Array(10,17,3,4,5,6,7,8,1001,6,2,100,2000,30,40),2);
    log.error(&quot;NumPartitions:: &quot;+data.getClass.toString+&quot; &quot;+data.getNumPartitions.toString)
    val data2 = sc.parallelize(Array(10,17,3,4,5,6,7,8,1001,6,2,100,2000,30,40));
    log.error(&quot;NumPartitions:: &quot;+data2.getClass.toString+&quot; &quot;+data2.getNumPartitions.toString)

    /**
      * 使用Reduce
      * def reduce(f: (T, T) ⇒ T): T
      * Reduces the elements of this RDD using the specified commutative and associative binary operator.
      * reduce方法是RDD结构的高级算子,数据处理中使用频率很高
      * 处理逻辑：
      * each partition is processed sequentially element by element
      * multiple partitions can be processed at the same time either by a single worker
      * (multiple executor threads) or different workers
      * partial results are fetched to the driver where the final reduction is applied
      */

    val max = data2.reduce((a,b) =&gt; Math.max(a,b))
    val min = data2.reduce((a,b) =&gt; Math.min(a,b))
    println(&quot;max : &quot; + max)
    println(&quot;min : &quot; + min)


    /**
      * 使用ReduceByKey
      * ReduceByKey属于PairRDD操作,对数据分区的把控能力更强
      *
      * def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope {
      * combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)
      * }
      *
      * /**
      * * Merge the values for each key using an associative and commutative reduce function. This will
      * * also perform the merging locally on each mapper before sending results to a reducer, similarly
      * * to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with numPartitions partitions.
      **/
      * def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)] = self.withScope {
      * reduceByKey(new HashPartitioner(numPartitions), func)
      * }
      */

    /**
      * Spark分区默认hashPartitioner,还可以使用rangePartitioner和自定义partitioner
      * https://blog.csdn.net/high2011/article/details/68491115
      */
    def putKey(iter: Iterator[Int]): Iterator[(String,Int)]= {
      var res = List[(String,Int)]()
      while (iter.hasNext) {
        val cur = iter.next;
        val new_value = ((cur%5+'a').toChar.toString,cur)
        res = new_value :: res
      }
      res.iterator
    }
    val data3 = data2.mapPartitions(putKey)

    data3.groupByKey().foreachPartition { p =&gt;
      println(&quot;分区&quot;+p.toIterator.mkString)
    }


    val maxByKey = data3.reduceByKey((a,b) =&gt; Math.max(a,b)).map(r=&gt;(&quot;last&quot;,r._2))
      .reduceByKey((a,b) =&gt; Math.max(a,b))
    val minByKey = data3.reduceByKey((a,b) =&gt; Math.min(a,b)).map(r=&gt;(&quot;last&quot;,r._2))
        .reduceByKey((a,b) =&gt; Math.min(a,b))

    println()
    println(&quot;maxByKey : &quot; + maxByKey.map(_._2).collect().mkString)
    println(&quot;minByKey : &quot; + minByKey.map(_._2).collect().mkString)

    /**
      * /**
      * * Aggregate the elements of each partition, and then the results for all the partitions, using
      * * given combine functions and a neutral &quot;zero value&quot;. This function can return a different result
      * * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U
      * * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are
      * * allowed to modify and return their first argument instead of creating a new U to avoid memory
      * * allocation.
      * *
      * * @param zeroValue the initial value for the accumulated result of each partition for the
      * *                  `seqOp` operator, and also the initial value for the combine results from
      * *                  different partitions for the `combOp` operator - this will typically be the
      * *                  neutral element (e.g. `Nil` for list concatenation or `0` for summation)
      * * @param seqOp an operator used to accumulate results within a partition
      * * @param combOp an associative operator used to combine results from different partitions
      **/
      * def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U
      *
      * aggregate方法
      *
      */
    val agrMax = data3.aggregate(Integer.MIN_VALUE)((maxValue,num) =&gt;Math.max(maxValue,num._2),
      (maxV1,maxV2)=&gt; Math.max(maxV1,maxV2))
    println(&quot;agrMax: &quot;+agrMax)

    //aggregationByKey
    val agrMaxByKey = data3.aggregateByKey(Integer.MIN_VALUE,4)((maxValue,num) =&gt;{Math.max(maxValue,num)},
      (maxV1,maxV2)=&gt; Math.max(maxV1,maxV2)
    )

    agrMaxByKey.foreach(print(_))
    println()

    val agrMaxKey = agrMaxByKey.map(r=&gt;(&quot;last&quot;,r._2))
      .reduceByKey((a,b) =&gt; Math.max(a,b)).map(_._2).collect().mkString

    println(&quot;maxByKey : &quot; + agrMaxKey)

    sc.stop
  }



  def deduplication(): Unit ={

    /**
      * ReduceByKey去重
      */
    val duplicateData = sc.parallelize(Array((&quot;2012-3-1&quot;,&quot;a&quot;),(&quot;2012-3-1&quot;,&quot;a&quot;),(&quot;2012-3-2&quot;,&quot;c&quot;),(&quot;2012-3-2&quot;,&quot;b&quot;),(&quot;2012-3-3&quot;,&quot;c&quot;)))

    println(duplicateData.map(p =&gt;(p,1)).reduceByKey(_+_).keys.collect().mkString)

    duplicateData.sortByKey()
  }


  def topN(): Unit ={
    val data = sc.parallelize(Array(
      (&quot;uuid_1&quot;,&quot;app_1&quot;,78),
      (&quot;uuid_1&quot;,&quot;app_2&quot;,90),
      (&quot;uuid_1&quot;,&quot;app_3&quot;,200),
      (&quot;uuid_2&quot;,&quot;app_1&quot;,3),
      (&quot;uuid_2&quot;,&quot;app_2&quot;,7),
      (&quot;uuid_3&quot;,&quot;app_1&quot;,178),
      (&quot;uuid_3&quot;,&quot;app_2&quot;,28),
      (&quot;uuid_3&quot;,&quot;app_3&quot;,43),
      (&quot;uuid_3&quot;,&quot;app_4&quot;,2),
      (&quot;uuid_4&quot;,&quot;app_1&quot;,0),
      (&quot;uuid_4&quot;,&quot;app_2&quot;,6),
      (&quot;uuid_4&quot;,&quot;app_3&quot;,743),
      (&quot;uuid_4&quot;,&quot;app_4&quot;,34)
    )
    )
    val data1 = data.map(p=&gt;((Random.nextInt(3),p._1),(p._2,p._3))).groupByKey()
    data1.foreach(p =&gt;println(p))
    println(&quot;==============&quot;)
//    val data2 = data1.flatMap({p=&gt;(p._1._2,p._2.toList.sortWith(_._2&gt;_._2).take(1))})
//    data2.foreach(p =&gt;println(p))
//    print(data2.collect().mkString)


    val topNResult3 = data.map(p=&gt;(p._1,(p._2,p._3))).aggregateByKey(ArrayBuffer[(String,Int)]())(
      (array, num) =&gt; {
        array += num
        array.sortWith(_._2&gt;_._2).takeRight(2)
      },
      (u1, u2) =&gt; {
        //对任意的两个局部聚合值进行聚合操作，可以会发生在combiner阶段和shuffle之后的最终的数据聚合的阶段
        u1 ++= u2
        u1.sorted.takeRight(2)
      }
    ).map(p =&gt; (p._1, p._2.toList))

    println(&quot;+---+---+ 使用aggregateByKey获取TopN的结果：&quot;)
    topNResult3.foreach(println(_))

  }

  def sparkConfig: SparkConf = {
    val conf = new SparkConf()
    conf.set(&quot;spark.streaming.kafka.consumer.cache.enabled&quot;, &quot;false&quot;)
    conf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;, &quot;8000&quot;)
    conf.set(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)
    conf.set(&quot;spark.streaming.backpressure.enabled&quot;, &quot;true&quot;)
    conf.set(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)
    conf.set(&quot;spark.scheduler.mode&quot;, &quot;FIFO&quot;)
    conf.set(&quot;spark.streaming.concurrentJobs&quot;, &quot;100&quot;)
    conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)
    conf.set(&quot;spark.debug.maxToStringField&quot;, &quot;1000&quot;)
    conf.set(&quot;spark.rdd.compress&quot;, &quot;true&quot;)
    conf.set(&quot;spark.streaming.kafka.consumer.cache.enabled&quot;, &quot;false&quot;)
    conf.set(&quot;spark.storage.memoryFraction&quot;, &quot;0.3&quot;)
  }
}
</code></pre>

        </div>

        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://bradley-db.github.io/tag/6XdFTjK9Q">
            <span class="flex-auto">大数据</span>
          </a>
        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://bradley-db.github.io/tag/xUjMcJ0rp">
            <span class="flex-auto">经验</span>
          </a>
        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://bradley-db.github.io/tag/Jz-liOFl9">
            <span class="flex-auto">工程技巧</span>
          </a>
        


        <div class="flex justify-between py-8">
          
            <div class="prev-post">
              <a href="https://bradley-db.github.io/post/2019-05-01-Mac-ShotCuts">
                <h3 class="post-title">
                  <i class="ri-arrow-left-line"></i>
                  Mac 快捷键整理
                </h3>
              </a>
            </div>
          

          
            <div class="next-post">
              <a href="https://bradley-db.github.io/post/2019-03-25-cognitive-study">
                <h3 class="post-title">
                  认知学习：如何转变职业生涯
                  <i class="ri-arrow-right-line"></i>
                </h3>
              </a>
            </div>
          
        </div>

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'a7cdf6a4218738b4f423',
    clientSecret: '04dfdd7b92da99e863c7e0396c204b47c3f13ebe',
    repo: 'blog-comment',
    owner: 'Bradley-dufe',
    admin: ['Bradley-dufe'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

      </div>
    </div>

    <script src="https://bradley-db.github.io/media/prism.js"></script>  
<script>

Prism.highlightAll()
let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

// This should probably be throttled.
// Especially because it triggers during smooth scrolling.
// https://lodash.com/docs/4.17.10#throttle
// You could do like...
// window.addEventListener("scroll", () => {
//    _.throttle(doThatStuff, 100);
// });
// Only not doing it here to keep this Pen dependency-free.

window.addEventListener("scroll", event => {
  let fromTop = window.scrollY;

  mainNavLinks.forEach((link, index) => {
    let section = document.getElementById(decodeURI(link.hash).substring(1));
    let nextSection = null
    if (mainNavLinks[index + 1]) {
      nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
    }
    if (section.offsetTop <= fromTop) {
      if (nextSection) {
        if (nextSection.offsetTop > fromTop) {
          link.classList.add("current");
        } else {
          link.classList.remove("current");    
        }
      } else {
        link.classList.add("current");
      }
    } else {
      link.classList.remove("current");
    }
  });
});


document.addEventListener("DOMContentLoaded", function() {
  var lazyImages = [].slice.call(document.querySelectorAll(".post-feature-image.lazy"));

  if ("IntersectionObserver" in window) {
    let lazyImageObserver = new IntersectionObserver(function(entries, observer) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          let lazyImage = entry.target
          lazyImage.style.backgroundImage = `url(${lazyImage.dataset.bg})`
          lazyImage.classList.remove("lazy")
          lazyImageObserver.unobserve(lazyImage)
        }
      });
    });

    lazyImages.forEach(function(lazyImage) {
      lazyImageObserver.observe(lazyImage)
    })
  } else {
    // Possibly fall back to a more compatible method here
  }
});

const menuContainer = document.querySelector('.menu-container')
const menus = document.querySelectorAll('.menu-container .menu')
const mask = document.querySelector('.mask')
const contentWrapper = document.querySelector('.content-wrapper')
const latestArticle = document.querySelector('.latest-article')
const readMore = document.querySelector('.read-more')
const indexPage = document.querySelector('.index-page')

const isHome = location.pathname === '/'
if (latestArticle) {
  latestArticle.style.display = isHome ? 'block' : 'none'
  readMore.style.display = isHome ? 'block' : 'none'
  indexPage.style.display = isHome ? 'none' : 'block'
}

const openMenu = () => {
  menuContainer.classList.add('open')
  menus.forEach(menu => {
    menu.classList.add('animated', 'fadeInLeft')
  })
  mask.classList.add('open')
  contentWrapper.classList.add('is-second')
}

const closeMenu = () => {
  menuContainer.classList.remove('open')
  menus.forEach(menu => {
    menu.classList.remove('animated', 'fadeInLeft')
  })
  mask.classList.remove('open')
  contentWrapper.classList.remove('is-second')
}
</script>
  
  </body>
</html>
